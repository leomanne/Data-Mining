{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fourth Task: Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import of the needed libraries and the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import tree\n",
    "\n",
    "import pydotplus\n",
    "import os\n",
    "\n",
    "from IPython.display import Image  \n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from joblib import dump\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cyclists = pd.read_csv('./dataset/cyclists_trasformed.csv')\n",
    "races = pd.read_csv('./dataset/races_trasformed.csv')\n",
    "\n",
    "os.makedirs('./models', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before employing the learning algorithms, we need to perform a few adjustments to our data. \n",
    "\n",
    "Firstly, it is necessary to merge the two dataset and delete not useful columns for the purpose.\n",
    "\n",
    "Then, we will add an attribute for specifying if a certain cyclist was in the first 20 position of a race or not. \n",
    "\n",
    "We delete:\n",
    "\n",
    "• *name* (coming from cyclist), since it is a repetition for *_url* in cyclist\n",
    "\n",
    "• *name* (coming from races), since it is a repetition for *_url* in races\n",
    "\n",
    "• *weight* and *height* (coming from cyclist), since these characteristics are combined in the *bmi* feature we created \n",
    "\n",
    "• *avg_position* and *avg_delta* (coming from cyclist), since it is better to consider more precise columns like *position* (see next) and *delta* from races\n",
    "\n",
    "• *position* (coming from races) since we consider only the first 20 we will get from the new column \n",
    "\n",
    "• *birth_year* (coming from cyclist) because it's redundant having *cyclist_age* from races\n",
    "\n",
    "• *cyclist_team* (coming from races) since we consider single cyclists\n",
    "\n",
    "• *date* (coming from races) whose format is: \"YYYY-MM-DD HH-MM-SS\". From it, we can extract the information we need: we exclude the \"HH-MM-SS\" and the \"MM-DD\" part because we can easily group races based on the *season* of the year (attribute that we already have). At the end, we only care about the year for splitting the races for the training set.\n",
    "\n",
    "We add:\n",
    "\n",
    "• *top_20*, having value =1 if the corresponding cyclist was in between these positions or, on the opposite, =0.\n",
    "\n",
    "**NB**: we drop *position* after creating the column *top_20*, since it is needed to fill the new one correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `races` and `cyclists` merge, matching rows where the `cyclist` column in `races` corresponds to the `_url` column in `cyclists`. The `indicator=True` argument adds a `_merge` column to indicate whether rows are present in both DataFrames, only in the left (`races`), or only in the right (`cyclists`).\n",
    "data_merged = pd.merge(races, cyclists, left_on='cyclist', right_on='_url', how='outer', indicator=True)\n",
    "\n",
    "# Filter non-corresponding rows: these cyclists never participate to a competition, so we don't consider them either for the prediction\n",
    "mismatched = data_merged[data_merged['_merge'] != 'both']\n",
    "print(f\"Number of mismatched entries: {len(mismatched)}\")\n",
    "print(mismatched)\n",
    "# Filter the merged DataFrame, `data_merged` to include only rows present in both the `races` and `cyclists` \n",
    "data_merged = data_merged[data_merged['_merge'] == 'both']\n",
    "\n",
    "# Drop the '_merge' column as it's no longer needed\n",
    "data_merged.drop('_merge', axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the specification, we create the new attribute *top_20* where the value is '1' if in the row the attribute *position* ranges from 0 to 19, '0' otherwise. \n",
    "\n",
    "As stated before, we drop *position* since is useless from now on and modify *date* so that it contains only the year. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename '_url_x' in 'race_url' and '_url_y' in 'cyclist_url'\n",
    "data_merged.rename(columns={'_url_x': 'race_url', '_url_y':'cyclist_url'}, inplace=True)\n",
    "\n",
    "# Delete useless columns we cited, except position\n",
    "data_merged = data_merged.drop(columns=['name_x', 'name_y', 'cyclist', 'weight', 'height', 'birth_year', 'avg_position', 'avg_delta', 'cyclist_team'])\n",
    "\n",
    "# Create 'top_20'\n",
    "data_merged['top_20'] = (data_merged['position'] < 20).astype(int)\n",
    "\n",
    "# Drop position \n",
    "data_merged = data_merged.drop(columns=['position']) \n",
    "\n",
    "data_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_merged.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learning algorithms require the categorical data to be transformed into numerical ones.\n",
    "\n",
    "In order to do this, we define the following function that assign a number for each different value inside the \n",
    "attribute starting from 1 and substitutes the categorical.  \n",
    "\n",
    "We cast the boolean values for *is_tarmac* to int. \n",
    "\n",
    "At the end, we are ready to define our 'train_set' and 'test_set' variables based on the year we get from *date*:\n",
    "\n",
    "- Training set: needed to train models.\n",
    "- Test set: need to test the model on never-seen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to discretize the variables\n",
    "# Input: the dataset and the list of variables' names to discretize\n",
    "def discretize_data(dataset, variables):\n",
    "    for variable in variables:\n",
    "        # Get the unique variable's values\n",
    "        var = sorted(dataset[variable].unique())\n",
    "        # Generate a mapping from the variable's values to the number representation  \n",
    "        mapping = dict(zip(var, range(0, len(var) + 1)))\n",
    "        dataset[variable] = dataset[variable].map(mapping).astype(int)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attribute to transform\n",
    "categorical_variables = ['race_url', 'season', 'cyclist_url', 'nationality', 'continent']\n",
    "data_merged = discretize_data(data_merged, categorical_variables)\n",
    "\n",
    "# Other casting\n",
    "data_merged['is_tarmac'] = data_merged['is_tarmac'].astype(int)\n",
    "data_merged['date'] = pd.DatetimeIndex(data_merged['date']).year\n",
    "\n",
    "# Save the dataset\n",
    "data_merged.to_csv('./dataset/data_merged.csv', index=False)\n",
    "\n",
    "# Training set\n",
    "train_data = data_merged[data_merged['date'] < 2022]\n",
    "# Test set\n",
    "test_data = data_merged[data_merged['date'] >= 2022]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our machine learning purpose, the dataset has to be divided into two parts:\n",
    "\n",
    "- Features: The input data containing the information needed by the model to make predictions (every attribute except *top_20*)\n",
    "- Target: The output data you want the model to predict (precisely *top_20*).\n",
    "\n",
    "So, we create two variables for both *train_data* and *test_data*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature and target for training\n",
    "train_feature = train_data.drop(columns=['top_20'])\n",
    "train_target = train_data['top_20']\n",
    "\n",
    "# Feature and target for testing\n",
    "test_feature = test_data.drop(columns=['top_20'])\n",
    "test_target = test_data['top_20']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The models we choose can be divided in different categories:\n",
    "\n",
    "1) Tree-Based Models (Decision Tree, Random Forest)\n",
    "\n",
    "2) AdaBoost\n",
    "\n",
    "3) Naïve Bayes\n",
    "\n",
    "4) K-Nearest Neighbors (KNN)\n",
    "\n",
    "5) Neural Network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`NB`: We tried to implement XGBoost, Rule-Based and SVM but after 15 minutes the methods were still running. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_scores(test_label, test_pred):\n",
    "    print(classification_report(test_label, \n",
    "                            test_pred, \n",
    "                            target_names=['Non-Top 20', 'Top 20']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree-Based Models (Decision Tree, Random Forest, AdaBoost)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating and configuring the Decision Tree\n",
    "dt = tree.DecisionTreeClassifier(\n",
    "    criterion='gini',               # Use the Gini index to evaluate the purity of splits\n",
    "    splitter='best',                # Splits the nodes by choosing the best split\n",
    "    class_weight='balanced',        # Gives more weight to 1-class since it has less support\n",
    "    max_depth=5,                    # Limit tree depth to 5 levels\n",
    "    min_samples_split=3,            # A node must have at least 3 samples to be split\n",
    "    min_samples_leaf=4,             # Each leaf must contain at least 4 examples\n",
    "    random_state=42                 # Ensures repeatability of results\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "dt = dt.fit(train_feature, train_target)\n",
    "\n",
    "# Save the model\n",
    "dump(dt, './models/decision_tree.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing the actual Decision Tree obtained: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To visualize the Decision Tree, you must intsall GraphViz\n",
    "#MacOs: brew install graphviz\n",
    "#Linux: sudo apt-get install graphviz\n",
    "#Windows: Install from here https://graphviz.org/download/ \n",
    "#and add the following enviroment variable (the path can change)\n",
    "#import os\n",
    "#os.environ[\"PATH\"] += os.pathsep + 'C:\\Program Files (x86)\\Graphviz2.38/bin/'\n",
    "\n",
    "dot_data = tree.export_graphviz(dt, out_file=None, \n",
    "                                feature_names=list(train_feature.columns),  \n",
    "                                class_names=['Non-Top 20', 'Top 20'],  \n",
    "                                filled=True, rounded=True)  \n",
    "graph = pydotplus.graph_from_dot_data(dot_data)  \n",
    "Image(graph.create_png())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction over test dataset employing Decision Tree\n",
    "test_pred_dt = dt.predict(test_feature)    \n",
    "\n",
    "# Compute the performance of the model\n",
    "report_scores(test_target, test_pred_dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to read the result:\n",
    "- **Non-Top 20**: \n",
    "    - Precision: 0.95 - Of all the predictions that the model classified as ‘Non-Top 20’, 95% were correct.\n",
    "    - Recall: 0.75: 75% of the riders actually ‘Non-Top 20’ were correctly identified by the model.\n",
    "    - F1-Score: 0.84 - Represents the balance between precision and recall, and is very high for this class, indicating that the model is excellent at correctly distinguishing ‘Non-Top 20’ cyclists.\n",
    "    - Support: 30.466 - Indicates the total number of true samples belonging to the ‘Non-Top 20’ class.\n",
    "- **Top 20**: \n",
    "    - Precision: 0.33 - Of all predictions classified as ‘Top 20’, only 33% are correct. This indicates that the model tends to include false positives.\n",
    "    - Recall: 0.77 - 77% of the cyclists actually in the ‘Top 20’ were recognised correctly. \n",
    "    - F1-Score: 0.46 - Being the balance between precision and recall, the mid-low value suggests that the model has difficulty with the ‘Top 20’ class.\n",
    "    - Support: 4.940 - Indicates the total number of true samples belonging to the ‘Top 20’ class.\n",
    "- **Accuracy** \n",
    "    - Accuracy: 0.75 - Percentage of correct predictions out of the total data. Although the value is high, it is influenced by the strong dominance of the Non-Top 20 class (majority class).\n",
    "- **Macro Average**\n",
    "    - Precision: 0.64 - Arithmetic mean of the precision of the two classes.\n",
    "    - Recall: 0.76 - Arithmetic mean of the recall of the two classes. Low due to extremely low recall for the ‘Top 20’ class.\n",
    "    - F1-Score: 0.65 - Arithmetic mean of the F1-Score of the two classes. Reflects the difficulty of the model in handling the ‘Top 20’ class.\n",
    "- **Weighted Average**.\n",
    "    - Precision: 0.87 - Weighted average of the precision, considering the support (size) of each class.\n",
    "    - Recall: 0.75 - Weighted average of recall, strongly influenced by the high recall of the ‘Non-Top 20’ class.\n",
    "    - F1-Score: 0.79 - Weighted average of the F1-Score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and visualize Confusion Matrix\n",
    "cm = confusion_matrix(test_target, test_pred_dt)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=dt.classes_)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to read the results:\n",
    "\n",
    "- True Negatives (TN): 22794 - Correct predictions for class 0.\n",
    "\n",
    "- False Positives (FP): 7672 - Incorrect predictions that indicated 1 instead of 0.\n",
    "\n",
    "- False Negatives (FN): 1132 - Incorrect predictions that indicated 0 instead of 1.\n",
    "\n",
    "- True Positives (TP): 3808 - Correct predictions for class 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest definition and training\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=100,         # Number of trees in the forest\n",
    "    criterion='gini',         # Measure to evaluate the purity of the split\n",
    "    class_weight='balanced',  # Gives more weight to 1-class since it has less support\n",
    "    max_depth=10,             # Maximum depth of trees\n",
    "    min_samples_split=5,      # Minimum number of samples to split a node\n",
    "    random_state=42,          # Ensures repeatability of results\n",
    ")\n",
    "\n",
    "# Model training\n",
    "rf = rf.fit(train_feature, train_target)\n",
    "\n",
    "# Save the model\n",
    "dump(rf, './models/random_forest.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction employing Random Forest\n",
    "test_pred_rf = rf.predict(test_feature)    \n",
    "\n",
    "# Compute the performance of the model\n",
    "report_scores(test_target, test_pred_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to read the result:\n",
    "- **Non-Top 20**: \n",
    "    - Precision: 0.96 - Of all the predictions that the model classified as ‘Non-Top 20’, 88% were correct.\n",
    "    - Recall: 0.81: 81% of the riders actually ‘Non-Top 20’ were correctly identified by the model.\n",
    "    - F1-Score: 0.88 - Represents the balance between precision and recall, and is very high for this class, indicating that the model is excellent at correctly distinguishing ‘Non-Top 20’ cyclists.\n",
    "    - Support: 30.466 - Indicates the total number of true samples belonging to the ‘Non-Top 20’ class.\n",
    "- **Top 20**: \n",
    "    - Precision: 0.40 - Of all predictions classified as ‘Top 20’, only 40% are correct. This indicates that the model tends to include false positives.\n",
    "    - Recall: 0.78 - 78% of the cyclists actually in the ‘Top 20’ were recognised correctly. \n",
    "    - F1-Score: 0.53 - Being the balance between precision and recall, the mid-low value suggests that the model has difficulty with the ‘Top 20’ class.\n",
    "    - Support: 4.940 - Indicates the total number of true samples belonging to the ‘Top 20’ class.\n",
    "- **Accuracy** \n",
    "    - Accuracy: 0.81 - Percentage of correct predictions out of the total data. Although the value is high, it is influenced by the strong dominance of the Non-Top 20 class (majority class).\n",
    "- **Macro Average**\n",
    "    - Precision: 0.68 - Arithmetic mean of the precision of the two classes.\n",
    "    - Recall: 0.80 - Arithmetic mean of the recall of the two classes. \n",
    "    - F1-Score: 0.70 - Arithmetic mean of the F1-Score of the two classes. \n",
    "- **Weighted Average**.\n",
    "    - Precision: 0.88 - Weighted average of the precision, considering the support (size) of each class.\n",
    "    - Recall: 0.81 - Weighted average of recall, strongly influenced by the high recall of the ‘Non-Top 20’ class.\n",
    "    - F1-Score: 0.83 - Weighted average of the F1-Score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and visualize Confusion Matrix\n",
    "cm = confusion_matrix(test_target, test_pred_rf)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=rf.classes_)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to read the results:\n",
    "\n",
    "- True Negatives (TN): 24635 - Correct predictions for class 0.\n",
    "\n",
    "- False Positives (FP): 5831 - Incorrect predictions that indicated 1 instead of 0.\n",
    "\n",
    "- False Negatives (FN): 1065 - Incorrect predictions that indicated 0 instead of 1.\n",
    "\n",
    "- True Positives (TP): 3875 - Correct predictions for class 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final comparisons\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary containing trained models\n",
    "models = {\n",
    "    \"Decision Tree\": dt,          \n",
    "    \"Random Forest\": rf   \n",
    "}\n",
    "\n",
    "# Iteration on models to calculate and print metrics\n",
    "for model_name, model in models.items():\n",
    "    test_pred = model.predict(test_feature)\n",
    "    acc = accuracy_score(test_target, test_pred)\n",
    "    conf = confusion_matrix(test_target, test_pred)\n",
    "    report = classification_report(test_target, test_pred, target_names=[\"Non Top 20\", \"Top 20\"], zero_division=0)\n",
    "    print(f\"\\n--- {model_name} ---\")\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(f\"Confusion Matrix:\\n{conf}\")\n",
    "    print(f\"Classification Report:\\n{report}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two versions of the AdaBoost have been implemented. \n",
    "\n",
    "The first one considers its basics features, so:\n",
    "- *base*: Decision Tree with height = 1 \n",
    "- *n_estimators*: 50\n",
    "- *learning rate*: 1\n",
    "\n",
    "Notice this method is less complex and fast to implement, but the low number of estimator and the high learning rate could lead to imprecise or tendentially wrong results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = AdaBoostClassifier()\n",
    "clf.fit(train_feature, train_target)\n",
    "\n",
    "# Save the model\n",
    "dump(clf, './models/adaboost.joblib')\n",
    "\n",
    "test_pred_clf = clf.predict(test_feature)\n",
    "print(classification_report(test_target, test_pred_clf, target_names=['Non-Top 20', 'Top 20']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to read the result:\n",
    "- **Non-Top 20**: \n",
    "    - Precision: 0.87 - Of all the predictions that the model classified as ‘Non-Top 20’, 87% were correct.\n",
    "    - Recall: 1.00: 100% of the riders actually ‘Non-Top 20’ were correctly identified by the model.\n",
    "    - F1-Score: 0.93 - Represents the balance between precision and recall, and is very high for this class, indicating that the model is excellent at correctly distinguishing ‘Non-Top 20’ cyclists.\n",
    "    - Support: 30.466 - Indicates the total number of true samples belonging to the ‘Non-Top 20’ class.\n",
    "- **Top 20**: \n",
    "    - Precision: 0.85 - Of all predictions classified as ‘Top 20’, 85% are correct. \n",
    "    - Recall: 0.05 - Only 5% of the cyclists actually in the ‘Top 20’ were recognised correctly. This indicates that the model is not effective in capturing true positives in this class.\n",
    "    - F1-Score: 0.10 - Being the balance between precision and recall, the low value suggests that the model has difficulty with the ‘Top 20’ class.\n",
    "    - Support: 4.940 - Indicates the total number of true samples belonging to the ‘Top 20’ class.\n",
    "- **Accuracy** \n",
    "    - Accuracy: 0.87 - Percentage of correct predictions out of the total data. Although the value is high, it is influenced by the strong dominance of the Non-Top 20 class (majority class).\n",
    "- **Macro Average**\n",
    "    - Precision: 0.86 - Arithmetic mean of the precision of the two classes.\n",
    "    - Recall: 0.53 - Arithmetic mean of the recall of the two classes. Low due to extremely low recall for the ‘Top 20’ class.\n",
    "    - F1-Score: 0.51 - Arithmetic mean of the F1-Score of the two classes. Reflects the difficulty of the model in handling the ‘Top 20’ class.\n",
    "- **Weighted Average**.\n",
    "    - Precision: 0.86 - Weighted average of the precision, considering the support (size) of each class.\n",
    "    - Recall: 0.87 - Weighted average of recall, strongly influenced by the high recall of the ‘Non-Top 20’ class.\n",
    "    - F1-Score: 0.81 - Weighted average of the F1-Score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and visualize Confusion Matrix\n",
    "cm = confusion_matrix(test_target, test_pred_clf)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=rf.classes_)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to read the results:\n",
    "\n",
    "- True Negatives (TN): 30421 - Correct predictions for class 0.\n",
    "\n",
    "- False Positives (FP): 45 - Incorrect predictions that indicated 1 instead of 0.\n",
    "\n",
    "- False Negatives (FN): 4677 - Incorrect predictions that indicated 0 instead of 1.\n",
    "\n",
    "- True Positives (TP): 263 - Correct predictions for class 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second versionm instead, employs the previously computed decision tree as a base, has a higher number of estimators (200) and a lower learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf2 = AdaBoostClassifier(estimator=dt, n_estimators=200, learning_rate=0.1)\n",
    "clf2.fit(train_feature, train_target)\n",
    "\n",
    "# Save the model\n",
    "dump(clf2, './models/adaboost2.joblib')\n",
    "\n",
    "test_pred_clf2 = clf2.predict(test_feature)\n",
    "print(classification_report(test_target, test_pred_clf2, target_names=['Non-Top 20', 'Top 20']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to read the result:\n",
    "- **Non-Top 20**: \n",
    "    - Precision: 0.96 - Of all the predictions that the model classified as ‘Non-Top 20’, 96% were correct.\n",
    "    - Recall: 0.75: 75% of the riders actually ‘Non-Top 20’ were correctly identified by the model.\n",
    "    - F1-Score: 0.84 - Represents the balance between precision and recall, and is very high for this class, indicating that the model is excellent at correctly distinguishing ‘Non-Top 20’ cyclists.\n",
    "    - Support: 30.466 - Indicates the total number of true samples belonging to the ‘Non-Top 20’ class.\n",
    "- **Top 20**: \n",
    "    - Precision: 0.34 - Of all predictions classified as ‘Top 20’, only 34% are correct. This indicates that the model tends to include false positives.\n",
    "    - Recall: 0.80 - 80% of the cyclists actually in the ‘Top 20’ were recognised correctly. \n",
    "    - F1-Score: 0.47 - Being the balance between precision and recall, the low value suggests that the model has difficulty with the ‘Top 20’ class.\n",
    "    - Support: 4.940 - Indicates the total number of true samples belonging to the ‘Top 20’ class.\n",
    "- **Accuracy** \n",
    "    - Accuracy: 0.75 - Percentage of correct predictions out of the total data. Although the value is high, it is influenced by the strong dominance of the Non-Top 20 class (majority class).\n",
    "- **Macro Average**\n",
    "    - Precision: 0.65 - Arithmetic mean of the precision of the two classes.\n",
    "    - Recall: 0.77 - Arithmetic mean of the recall of the two classes. Low due to extremely low recall for the ‘Top 20’ class.\n",
    "    - F1-Score: 0.66 - Arithmetic mean of the F1-Score of the two classes. Reflects the difficulty of the model in handling the ‘Top 20’ class.\n",
    "- **Weighted Average**.\n",
    "    - Precision: 0.87 - Weighted average of the precision, considering the support (size) of each class.\n",
    "    - Recall: 0.75 - Weighted average of recall, strongly influenced by the high recall of the ‘Non-Top 20’ class.\n",
    "    - F1-Score: 0.79 - Weighted average of the F1-Score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and visualize Confusion Matrix\n",
    "cm = confusion_matrix(test_target, test_pred_clf2)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=rf.classes_)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to read the results:\n",
    "\n",
    "- True Negatives (TN): 22710 - Correct predictions for class 0.\n",
    "\n",
    "- False Positives (FP): 7756 - Incorrect predictions that indicated 1 instead of 0.\n",
    "\n",
    "- False Negatives (FN): 1001 - Incorrect predictions that indicated 0 instead of 1.\n",
    "\n",
    "- True Positives (TP): 3939 - Correct predictions for class 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naïve Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naïve Bayes training\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(train_feature, train_target)\n",
    "\n",
    "# Save the model\n",
    "dump(gnb, './models/naive_bayes.joblib')\n",
    "\n",
    "# Prediction employing Naive Bayes\n",
    "test_pred_gnb = gnb.predict(test_feature)  \n",
    "\n",
    "# Compute the performance of the model\n",
    "report_scores(test_target, test_pred_gnb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to read the result:\n",
    "- **Non-Top 20**: \n",
    "    - Precision: 0.88 - Of all the predictions that the model classified as ‘Non-Top 20’, 88% were correct.\n",
    "    - Recall: 0.88: 88% of the riders actually ‘Non-Top 20’ were correctly identified by the model.\n",
    "    - F1-Score: 0.88 - Represents the balance between precision and recall, and is very high for this class, indicating that the model is excellent at correctly distinguishing ‘Non-Top 20’ cyclists.\n",
    "    - Support: 30.466 - Indicates the total number of true samples belonging to the ‘Non-Top 20’ class.\n",
    "- **Top 20**: \n",
    "    - Precision: 0.24 - Of all predictions classified as ‘Top 20’, only 33% are correct. This indicates that the model tends to include false positives.\n",
    "    - Recall: 0.23 - 77% of the cyclists actually in the ‘Top 20’ were recognised correctly. \n",
    "    - F1-Score: 0.23 - Being the balance between precision and recall, the mid-low value suggests that the model has difficulty with the ‘Top 20’ class.\n",
    "    - Support: 4.940 - Indicates the total number of true samples belonging to the ‘Top 20’ class.\n",
    "- **Accuracy** \n",
    "    - Accuracy: 0.79 - Percentage of correct predictions out of the total data. Although the value is high, it is influenced by the strong dominance of the Non-Top 20 class (majority class).\n",
    "- **Macro Average**\n",
    "    - Precision: 0.56 - Arithmetic mean of the precision of the two classes.\n",
    "    - Recall: 0.56 - Arithmetic mean of the recall of the two classes. Low due to extremely low recall for the ‘Top 20’ class.\n",
    "    - F1-Score: 0.56 - Arithmetic mean of the F1-Score of the two classes. Reflects the difficulty of the model in handling the ‘Top 20’ class.\n",
    "- **Weighted Average**.\n",
    "    - Precision: 0.79 - Weighted average of the precision, considering the support (size) of each class.\n",
    "    - Recall: 0.79 - Weighted average of recall, strongly influenced by the high recall of the ‘Non-Top 20’ class.\n",
    "    - F1-Score: 0.79 - Weighted average of the F1-Score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and visualize Confusion Matrix\n",
    "cm = confusion_matrix(test_target, test_pred_gnb)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=dt.classes_)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to read the results:\n",
    "\n",
    "- True Negatives (TN): 26782 - Correct predictions for class 0.\n",
    "\n",
    "- False Positives (FP): 3684 - Incorrect predictions that indicated 1 instead of 0.\n",
    "\n",
    "- False Negatives (FN): 3795 - Incorrect predictions that indicated 0 instead of 1.\n",
    "\n",
    "- True Positives (TP): 1145 - Correct predictions for class 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors (KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of KNN model with 3 neighbours and ‘ball_tree’ algorithm\n",
    "knn = KNeighborsClassifier(n_neighbors=3, algorithm='ball_tree', metric='minkowski')\n",
    "\n",
    "# Train KNN on the training data\n",
    "knn.fit(train_feature, train_target) \n",
    "\n",
    "# Save the model\n",
    "dump(knn, './models/knn.joblib')\n",
    "\n",
    "# Prediction\n",
    "test_pred_knn = knn.predict(test_feature)    # Predictions on the test set\n",
    "\n",
    "# Compute the performance of the model\n",
    "report_scores(test_target, test_pred_knn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to read the result:\n",
    "- **Non-Top 20**: \n",
    "    - Precision: 0.90 - Of all the predictions that the model classified as ‘Non-Top 20’, 90% were correct.\n",
    "    - Recall: 0.86: 86% of the riders actually ‘Non-Top 20’ were correctly identified by the model.\n",
    "    - F1-Score: 0.88 - Represents the balance between precision and recall, and is very high for this class, indicating that the model is excellent at correctly distinguishing ‘Non-Top 20’ cyclists.\n",
    "    - Support: 30.466 - Indicates the total number of true samples belonging to the ‘Non-Top 20’ class.\n",
    "- **Top 20**: \n",
    "    - Precision: 0.30 - Of all predictions classified as ‘Top 20’, only 63% are correct. This indicates that the model tends to include false positives.\n",
    "    - Recall: 0.38 - Only 38% of the cyclists actually in the ‘Top 20’ were recognised correctly. This indicates that the model is not effective in capturing true positives in this class.\n",
    "    - F1-Score: 0.34 - Being the balance between precision and recall, the low value suggests that the model has difficulty with the ‘Top 20’ class.\n",
    "    - Support: 4.940 - Indicates the total number of true samples belonging to the ‘Top 20’ class.\n",
    "- **Accuracy** \n",
    "    - Accuracy: 0.79 - Percentage of correct predictions out of the total data. Although the value is high, it is influenced by the strong dominance of the Non-Top 20 class (majority class).\n",
    "- **Macro Average**\n",
    "    - Precision: 0.60 - Arithmetic mean of the precision of the two classes.\n",
    "    - Recall: 0.62 - Arithmetic mean of the recall of the two classes. Low due to extremely low recall for the ‘Top 20’ class.\n",
    "    - F1-Score: 0.61 - Arithmetic mean of the F1-Score of the two classes. Reflects the difficulty of the model in handling the ‘Top 20’ class.\n",
    "- **Weighted Average**.\n",
    "    - Precision: 0.81 - Weighted average of the precision, considering the support (size) of each class.\n",
    "    - Recall: 0.79 - Weighted average of recall, strongly influenced by the high recall of the ‘Non-Top 20’ class.\n",
    "    - F1-Score: 0.80 - Weighted average of the F1-Score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and visualize Confusion Matrix\n",
    "cm = confusion_matrix(test_target, test_pred_knn)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=dt.classes_)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to read the results:\n",
    "\n",
    "- True Negatives (TN): 26211 - Correct predictions for class 0.\n",
    "\n",
    "- False Positives (FP): 4255 - Incorrect predictions that indicated 1 instead of 0.\n",
    "\n",
    "- False Negatives (FN): 3075 - Incorrect predictions that indicated 0 instead of 1.\n",
    "\n",
    "- True Positives (TP): 1865 - Correct predictions for class 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `train_target` is converted to a NumPy array of type `float32` and reshaped into a 2D array with one column. \n",
    "# `train_feature` is reshaped into a 3D array to match the expected input format for a neural network (samples, timesteps, features).\n",
    "\n",
    "y_train = np.asarray(train_target).astype('float32').reshape((-1,1))\n",
    "x_train = np.reshape(train_feature.values, (train_feature.shape[0], 1, train_feature.shape[1]))\n",
    "\n",
    "print(y_train.shape)\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This block defines a Sequential neural network model using TensorFlow:\n",
    "   # - The input layer expects data with a shape of `(1, 17)`\n",
    "   # - A `Flatten` layer reduces the dimensions to a single vector.\n",
    "   # - Two dense (fully connected) layers, each with 256 units and a sigmoid activation function, are included, followed by a 10% dropout layer to prevent overfitting.\n",
    "   # - The final dense layer outputs a single value with a sigmoid activation function, suitable for binary classification.\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Input(shape=(1, 17)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(256, activation='sigmoid'),\n",
    "    tf.keras.layers.Dropout(0.1),\n",
    "    tf.keras.layers.Dense(256, activation='sigmoid'),\n",
    "    tf.keras.layers.Dropout(0.1),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Model compilation\n",
    "model.compile(optimizer='adamax',  # Adamax optimizer\n",
    "              loss='binary_crossentropy',  # Binary cross-entropy for classification (appropriate for binary classification tasks)\n",
    "              metrics=['accuracy']) # Accuracy as the evaluation metric\n",
    "\n",
    "history = model.fit(\n",
    "    x_train, y_train,\n",
    "    epochs=20,            # The model will go through the entire training dataset 20 times\n",
    "    batch_size=256,       # The training data will be divided into batches of 256 samples\n",
    "    validation_split=0.2  # Reserves 20% of the training data for validation\n",
    ")\n",
    "\n",
    "# Save the model\n",
    "model.save('./models/neural_network.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training and validation accuracy\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(acc) + 1)\n",
    "plt.plot(epochs, acc, 'bo', label='Training Acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation Acc')\n",
    "plt.title('Training and validation Acc')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Acc')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "x_test = np.reshape(test_feature.values, (test_feature.shape[0], 1, test_feature.shape[1]))\n",
    "test_pred = (model.predict(x_test) > 0.5).astype('int32')\n",
    "\n",
    "# Compute the performance of the model\n",
    "report_scores(test_target, test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to read the result:\n",
    "- **Non-Top 20**: \n",
    "    - Precision: 0.87 - Of all the predictions that the model classified as ‘Non-Top 20’, 87% were correct.\n",
    "    - Recall: 1.00: 100% of the riders actually ‘Non-Top 20’ were correctly identified by the model.\n",
    "    - F1-Score: 0.93 - Represents the balance between precision and recall, and is very high for this class, indicating that the model is excellent at correctly distinguishing ‘Non-Top 20’ cyclists.\n",
    "    - Support: 30.466 - Indicates the total number of true samples belonging to the ‘Non-Top 20’ class.\n",
    "- **Top 20**: \n",
    "    - Precision: 0.76 - Of all predictions classified as ‘Top 20’, 76% is correct. \n",
    "    - Recall: 0.06 - 6% of the cyclists actually in the ‘Top 20’ were recognised correctly. The model is not effective in capturing true positives in this class.\n",
    "    - F1-Score: 0.11 - Being the balance between precision and recall, the low value suggests that the model has difficulty with the ‘Top 20’ class.\n",
    "    - Support: 4.940 - Indicates the total number of true samples belonging to the ‘Top 20’ class.\n",
    "- **Accuracy** \n",
    "    - Accuracy: 0.87 - Percentage of correct predictions out of the total data. Although the value is high, it is influenced by the strong dominance of the Non-Top 20 class (majority class).\n",
    "- **Macro Average**\n",
    "    - Precision: 0.81 - Arithmetic mean of the precision of the two classes.\n",
    "    - Recall: 0.53 - Arithmetic mean of the recall of the two classes. Low due to extremely low recall for the ‘Top 20’ class.\n",
    "    - F1-Score: 0.52 - Arithmetic mean of the F1-Score of the two classes. Reflects the difficulty of the model in handling the ‘Top 20’ class.\n",
    "- **Weighted Average**.\n",
    "    - Precision: 0.85 - Weighted average of the precision, considering the support (size) of each class.\n",
    "    - Recall: 0.87 - Weighted average of recall, strongly influenced by the high recall of the ‘Non-Top 20’ class.\n",
    "    - F1-Score: 0.81 - Weighted average of the F1-Score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and visualize Confusion Matrix\n",
    "cm = confusion_matrix(test_target, test_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=dt.classes_)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to read the results:\n",
    "\n",
    "- True Negatives (TN): 30378 - Correct predictions for class 0.\n",
    "\n",
    "- False Positives (FP): 88 - Incorrect predictions that indicated 1 instead of 0.\n",
    "\n",
    "- False Negatives (FN): 4660 - Incorrect predictions that indicated 0 instead of 1.\n",
    "\n",
    "- True Positives (TP): 280 - Correct predictions for class 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison between all models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **ROC curve**: Displays the relationship between the True Positive Rate and the False Positive Rate. A curve closer to the upper left corner indicates better performance.\n",
    "\n",
    "- **AUC (Area Under Curve)**: A higher AUC value indicates a better predictive ability of the model. The maximum value is 1 (perfect classifier), while 0.5 indicates a random model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(0).clf()  # Clear the current figure\n",
    "\n",
    "# Decision Tree\n",
    "fpr, tpr, thresh = metrics.roc_curve(test_target, test_pred_dt)\n",
    "auc = metrics.roc_auc_score(test_target, test_pred_dt)\n",
    "plt.plot(fpr, tpr, label=\"DecisionTree, auc=\" + str(round(auc, 3)))\n",
    "\n",
    "# Random Forest\n",
    "fpr, tpr, thresh = metrics.roc_curve(test_target, test_pred_rf)\n",
    "auc = metrics.roc_auc_score(test_target, test_pred_rf)\n",
    "plt.plot(fpr, tpr, label=\"RandomForest, auc=\" + str(round(auc, 3)))\n",
    "\n",
    "# AdaBoost\n",
    "fpr, tpr, thresh = metrics.roc_curve(test_target, test_pred_clf)\n",
    "auc = metrics.roc_auc_score(test_target, test_pred_clf)\n",
    "plt.plot(fpr,tpr,label=\"AdaBoost, auc=\"+str(auc))\n",
    "\n",
    "# Enhanced AdaBoost\n",
    "fpr, tpr, thresh = metrics.roc_curve(test_target, test_pred_clf2)\n",
    "auc = metrics.roc_auc_score(test_target, test_pred_clf2)\n",
    "plt.plot(fpr,tpr,label=\"Enhanced AdaBoost, auc=\"+str(auc))\n",
    "\n",
    "# Naive Bayes\n",
    "fpr, tpr, thresh = metrics.roc_curve(test_target, test_pred_gnb)\n",
    "auc = metrics.roc_auc_score(test_target, test_pred_gnb)\n",
    "plt.plot(fpr, tpr, label=\"Naive Bayes, auc=\" + str(round(auc, 3)))\n",
    "\n",
    "\n",
    "# K-Nearest Neighbor\n",
    "fpr, tpr, thresh = metrics.roc_curve(test_target, test_pred_knn)\n",
    "auc = metrics.roc_auc_score(test_target, test_pred_knn)\n",
    "plt.plot(fpr, tpr, label=\"KNN, auc=\" + str(round(auc, 3)))\n",
    "\n",
    "# Neural Network\n",
    "test_pred_nn = model.predict(x_test).ravel()  # Predict probabilities for Neural Network\n",
    "fpr, tpr, thresh = metrics.roc_curve(test_target, test_pred_nn)\n",
    "auc = metrics.roc_auc_score(test_target, test_pred_nn)\n",
    "plt.plot(fpr, tpr, label=\"Neural Network, auc=\" + str(round(auc, 3)))\n",
    "\n",
    "# Layout\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Comparison of ROC Curves')\n",
    "plt.legend(loc=0)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
